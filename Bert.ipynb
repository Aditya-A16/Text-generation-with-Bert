{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "UTXWS9Ib86TY"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('deepset/bert-base-cased-squad2')\n",
        "model = BertForQuestionAnswering.from_pretrained('deepset/bert-base-cased-squad2')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTZNUPzS9Dqm",
        "outputId": "f072c80a-16ee-4861-b152-9da9106c23d7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at deepset/bert-base-cased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input text and question\n",
        "text = \"In a shocking turn of events, it was revealed that the once respected businessman had been embezzling funds from the company for years. Despite his deceit, many of his employees expressed their loyalty and support for him.\"\n",
        "question = \"Who was revealed to have been embezzling funds from the company?\"\n",
        "\n",
        "# Encode the input text and question, and get the scores for each word in the text\n",
        "input = tokenizer(question, text,  return_tensors=\"pt\")\n",
        "\n",
        "output = model(**input)\n",
        "\n",
        "# Find the words in the text that corresponds to the highest start and end scores\n",
        "# with a torch.no_grad():\n",
        "#     outputs = model(**inputs)\n",
        "start_index = output.start_logits.argmax()\n",
        "end_index = output.end_logits.argmax() + 1\n",
        "\n",
        "# Extract the span of words as the answer\n",
        "answer = tokenizer.decode(input.input_ids[0, start_index:end_index])\n",
        "print(answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tz7FpIOH9FSt",
        "outputId": "402f945d-6662-4772-ad98-892697feebcd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the once respected businessman\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"Hello, how can I help you today?\", \"Hi, I need some help with my order.\"),\n",
        "        (\"Sure, what seems to be the problem?\", \"I never received my order and it's been over a week.\"),\n",
        "        (\"I'm sorry to hear that. Let me check on that for you.\", \"Thank you. Can you also check on the status of my refund?\"),\n",
        "        (\"Certainly. I will check on that as well.\", \"Thank you. Can you also provide me with the contact information for your supervisor?\"),\n",
        "        (\"Of course. Here is the phone number and email address for our supervisor.\", \"Thank you for your help.\")]\n",
        "\n",
        "questions = [item[0] for item in data]\n",
        "answers = [item[1] for item in data]\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "# Set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# Convert the data to input format for BERT\n",
        "question_input_ids = []\n",
        "question_attention_masks = []\n",
        "for i in range(len(questions)):\n",
        "    # Tokenize the question\n",
        "    question_tokens = tokenizer.tokenize(questions[i])\n",
        "\n",
        "    max_length = 512\n",
        "    # Pad the input tokens\n",
        "    question_tokens = question_tokens + [tokenizer.pad_token] * (max_length - len(question_tokens))\n",
        "\n",
        "    # Create the input ids for the BERT model\n",
        "    question_input_ids.append(tokenizer.convert_tokens_to_ids(question_tokens))\n",
        "\n",
        "    # Create the attention masks for the input tokens\n",
        "    question_attention_masks.append([1 if token != tokenizer.pad_token else 0 for token in question_tokens])\n",
        "\n",
        "answer_input_ids = []\n",
        "answer_attention_masks = []\n",
        "for i in range(len(answers)):\n",
        "    # Tokenize the answer\n",
        "    answer_tokens = tokenizer.tokenize(answers[i])\n",
        "\n",
        "    # Pad the input tokens\n",
        "    answer_tokens = answer_tokens + [tokenizer.pad_token] * (max_length - len(answer_tokens))\n",
        "\n",
        "    # Create the input ids for the BERT model\n",
        "    answer_input_ids.append(tokenizer.convert_tokens_to_ids(answer_tokens))\n",
        "\n",
        "    # Create the attention masks for the input tokens\n",
        "    answer_attention_masks.append([1 if token != tokenizer.pad_token else 0 for token in answer_tokens])\n",
        "\n",
        "# Concatenate the question and answer input lists\n",
        "input_ids = question_input_ids + answer_input_ids\n",
        "attention_masks = question_attention_masks + answer_attention_masks\n",
        "\n",
        "# Convert the input ids and attention masks to tensors\n",
        "input_ids = torch.tensor(input_ids).to(device)\n",
        "attention_masks = torch.tensor(attention_masks).to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "SdZbUpHF9qVG"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
      ],
      "metadata": {
        "id": "Fulwa2CY_YIX"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the criterion\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def calculate_accuracy(predictions, labels):\n",
        "    correct_predictions = 0\n",
        "    for i in range(len(predictions)):\n",
        "        if predictions[i] == labels[i]:\n",
        "            correct_predictions += 1\n",
        "    return correct_predictions / len(predictions)\n",
        "\n",
        "# Set the number of epochs\n",
        "num_epochs = 10\n",
        "\n",
        "# Set the labels for the data\n",
        "labels = [0 if i < len(questions) else 1 for i in range(len(questions) + len(answers))]\n",
        "labels = torch.tensor(labels).to(device)\n",
        "\n",
        "# Set the training loop\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # Set the training mode\n",
        "    model.train()\n",
        "\n",
        "    # Clear the gradients\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    model = model.to(device)\n",
        "    output = model(input_ids, attention_mask=attention_masks)\n",
        "\n",
        "    # Calculate the loss\n",
        "    loss = criterion(output[0], labels)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the parameters\n",
        "    #torch.optimizer.OPTIMIZER.STEP()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print the loss and accuracy\n",
        "    print(\"Epoch {}/{} - Loss: {:.5f} - Accuracy: {:.5f}\".format(epoch + 1, num_epochs, loss.item(), calculate_accuracy(output[0].argmax(dim=1).cpu().numpy(), labels.cpu().numpy())))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd3DD5_89GzO",
        "outputId": "84e38d63-68f9-4da3-850e-11db4126eca7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Loss: 5.64962 - Accuracy: 0.00000\n",
            "Epoch 2/10 - Loss: 2.46114 - Accuracy: 0.60000\n",
            "Epoch 3/10 - Loss: 1.73734 - Accuracy: 0.80000\n",
            "Epoch 4/10 - Loss: 2.57871 - Accuracy: 0.50000\n",
            "Epoch 5/10 - Loss: 3.23318 - Accuracy: 0.40000\n",
            "Epoch 6/10 - Loss: 1.97133 - Accuracy: 0.50000\n",
            "Epoch 7/10 - Loss: 0.79754 - Accuracy: 0.80000\n",
            "Epoch 8/10 - Loss: 0.29000 - Accuracy: 0.80000\n",
            "Epoch 9/10 - Loss: 0.25680 - Accuracy: 1.00000\n",
            "Epoch 10/10 - Loss: 0.28785 - Accuracy: 0.90000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to eval mode\n",
        "model.eval()\n",
        "\n",
        "# Define the input\n",
        "input_text = \"Hello, how can I help you today?\"\n",
        "\n",
        "# Tokenize the input\n",
        "input_tokens = tokenizer.tokenize(input_text)\n",
        "\n",
        "# Pad the input tokens\n",
        "input_tokens = input_tokens + [tokenizer.pad_token] * (max_length - len(input_tokens))\n",
        "\n",
        "# Convert the input tokens to input ids\n",
        "input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
        "\n",
        "# Create the attention mask for the input\n",
        "attention_mask = [1 if token != tokenizer.pad_token else 0 for token in input_tokens]\n",
        "\n",
        "# Convert the input ids and attention mask to tensors\n",
        "input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
        "attention_mask = torch.tensor(attention_mask).unsqueeze(0).to(device)\n",
        "\n",
        "# Get the model output\n",
        "output = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "# Get the predicted label\n",
        "prediction = output[0].argmax(dim=1).item()\n",
        "\n",
        "# Print the output\n",
        "if prediction == 0:\n",
        "    print(\"Question: {}\".format(input_text))\n",
        "else:\n",
        "    print(\"Answer: {}\".format(answers[prediction - 1]))\n",
        "\n"
      ],
      "metadata": {
        "id": "E9J1xmES9Io2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit\n"
      ],
      "metadata": {
        "id": "W-5293cBIQK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st"
      ],
      "metadata": {
        "id": "D7O_gLNJIpLA"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "\n",
        "from transformers import BertTokenizer, BertForQuestionAnswering, BertForSequenceClassification\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "max_length = 512\n",
        "model = BertForQuestionAnswering.from_pretrained('deepset/bert-base-cased-squad2')\n",
        "model = model.to(device)\n",
        "data = [(\"Hello, how can I help you today?\", \"Hi, I need some help with my order.\"),\n",
        "        (\"Sure, what seems to be the problem?\", \"I never received my order and it's been over a week.\"),\n",
        "        (\"I'm sorry to hear that. Let me check on that for you.\", \"Thank you. Can you also check on the status of my refund?\"),\n",
        "        (\"Certainly. I will check on that as well.\", \"Thank you. Can you also provide me with the contact information for your supervisor?\"),\n",
        "        (\"Of course. Here is the phone number and email address for our supervisor.\", \"Thank you for your help.\")]\n",
        "\n",
        "questions = [item[0] for item in data]\n",
        "answers = [item[1] for item in data]\n",
        "print(answers)\n",
        "def get_answer(input_text):\n",
        "  # Define the input\n",
        "  input_text = input_text\n",
        "\n",
        "  # Tokenize the input\n",
        "  input_tokens = tokenizer.tokenize(input_text)\n",
        "\n",
        "  # Pad the input tokens\n",
        "  input_tokens = input_tokens + [tokenizer.pad_token] * (max_length - len(input_tokens))\n",
        "\n",
        "  # Convert the input tokens to input ids\n",
        "  input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
        "\n",
        "  # Create the attention mask for the input\n",
        "  attention_mask = [1 if token != tokenizer.pad_token else 0 for token in input_tokens]\n",
        "\n",
        "  # Convert the input ids and attention mask to tensors\n",
        "  input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
        "  attention_mask = torch.tensor(attention_mask).unsqueeze(0).to(device)\n",
        "\n",
        "  # Get the model output\n",
        "  output = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "  # Get the predicted label\n",
        "  prediction = output[0].argmax(dim=1).item()\n",
        "\n",
        "  # Print the output\n",
        "  if prediction == 0:\n",
        "      return (\"Question: {}\".format(input_text))\n",
        "  else:\n",
        "      return (\"Answer: {}\".format(answers[prediction - 1]))\n",
        "\n",
        "\n",
        "\n",
        "def generate_output(input_text):\n",
        "    # Modify this function to generate the desired output based on the input text.\n",
        "    # For demonstration purposes, let's just return the input text as output.\n",
        "    return get_answer(input_text)\n",
        "\n",
        "def main():\n",
        "    st.title(\"Text Generator\")\n",
        "\n",
        "    # Text input from the user\n",
        "    input_text = st.text_area(\"Enter some text:\", \"\")\n",
        "\n",
        "    if st.button(\"Generate\"):\n",
        "        if input_text:\n",
        "            # Call the function to generate the output\n",
        "            output_text = generate_output(input_text)\n",
        "            st.write(\"Generated Output:\")\n",
        "            st.write(output_text)\n",
        "        else:\n",
        "            st.warning(\"Please enter some text!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQy57w1vIV3n",
        "outputId": "4775888e-da85-4110-fb71-40e545c4fdc9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com\n",
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scF5BOmjRX6B",
        "outputId": "5478d679-498f-4d5a-9d69-2fdea57aaf98"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.223.42.208\n",
            "[..................] / rollbackFailedOptional: verb npm-session 0e99bc09fd7791a\u001b[0m\u001b[K\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.223.42.208:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.352s\n",
            "your url is: https://polite-clocks-yell.loca.lt\n",
            "Some weights of the model checkpoint at deepset/bert-base-cased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "['Hi, I need some help with my order.', \"I never received my order and it's been over a week.\", 'Thank you. Can you also check on the status of my refund?', 'Thank you. Can you also provide me with the contact information for your supervisor?', 'Thank you for your help.']\n",
            "Some weights of the model checkpoint at deepset/bert-base-cased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at deepset/bert-base-cased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "['Hi, I need some help with my order.', \"I never received my order and it's been over a week.\", 'Thank you. Can you also check on the status of my refund?', 'Thank you. Can you also provide me with the contact information for your supervisor?', 'Thank you for your help.']\n",
            "['Hi, I need some help with my order.', \"I never received my order and it's been over a week.\", 'Thank you. Can you also check on the status of my refund?', 'Thank you. Can you also provide me with the contact information for your supervisor?', 'Thank you for your help.']\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}